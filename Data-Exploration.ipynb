{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4225157188458270974, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9425665546910455468\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2304114688\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 14481774252847935662\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 4357901821301314906\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_cifar_data():\n",
    "    folder_path = 'cifar-10-batches-py'\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f'{folder_path}/data_batch_{i}')\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "    \n",
    "    return np.concatenate(tuple(data), axis=0), np.concatenate(tuple(labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_all_cifar_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((50000,10))\n",
    "a[np.arange(50000), labels] = 1\n",
    "\n",
    "labels = a\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.divide(data, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "from tensorflow.keras.layers import Dense\n",
    "model.add(Dense(units=1024, activation='relu', input_dim=3072))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, labels, validation_split=0.20, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('simple-nn-100-epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_for_cnn(data):\n",
    "    num_entries_in_channel = int(data.shape[1] / 3)\n",
    "    data_4d = np.zeros((50000,3,32,32))\n",
    "    for i in range(50000):\n",
    "        x = data[i]\n",
    "        r_1d = x[0:num_entries_in_channel]\n",
    "        g_1d = x[num_entries_in_channel:2*num_entries_in_channel]\n",
    "        b_1d = x[num_entries_in_channel*2:]\n",
    "\n",
    "        r_2d = np.reshape(r_1d, (32,32))\n",
    "        g_2d = np.reshape(g_1d, (32,32))\n",
    "        b_2d = np.reshape(b_1d, (32,32))\n",
    "        \n",
    "        image_3d = np.zeros((3,32,32))\n",
    "        image_3d[0] = r_2d\n",
    "        image_3d[1] = g_2d\n",
    "        image_3d[2] = b_2d\n",
    "                \n",
    "        data_4d[i] = image_3d\n",
    "    print(data_4d.shape)\n",
    "    data_4d = np.swapaxes(data_4d,1,3)\n",
    "    data_4d = np.swapaxes(data_4d, 1,2)\n",
    "    \n",
    "    mean = np.mean(data_4d,axis=(0,1,2,3))\n",
    "    std = np.std(data_4d,axis=(0,1,2,3))\n",
    "    data_4d = (data_4d-mean)/(std+1e-7)\n",
    "    return data_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=15, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('simple-cnn-model-100-epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN WITH SEPERABLE CONVOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, DepthwiseConv2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, kernel_size=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, kernel_size=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 16, 16, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 8, 8, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 8, 16)          528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 4, 4, 16)          160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 32)          544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 8,538\n",
      "Trainable params: 8,218\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 14s 344us/sample - loss: 1.7637 - acc: 0.3625 - val_loss: 1.5887 - val_acc: 0.4235\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 13s 334us/sample - loss: 1.3998 - acc: 0.4966 - val_loss: 1.3393 - val_acc: 0.5173\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 13s 325us/sample - loss: 1.2745 - acc: 0.5442 - val_loss: 1.2637 - val_acc: 0.5493\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 13s 313us/sample - loss: 1.1873 - acc: 0.5778 - val_loss: 1.2208 - val_acc: 0.5712\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 12s 310us/sample - loss: 1.1238 - acc: 0.6008 - val_loss: 1.1489 - val_acc: 0.5947\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 12s 311us/sample - loss: 1.0801 - acc: 0.6165 - val_loss: 1.1345 - val_acc: 0.6059\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 12s 309us/sample - loss: 1.0466 - acc: 0.6308 - val_loss: 1.0886 - val_acc: 0.6200\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 14s 352us/sample - loss: 1.0155 - acc: 0.6410 - val_loss: 1.0635 - val_acc: 0.6259\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 13s 331us/sample - loss: 0.9944 - acc: 0.6499 - val_loss: 1.0544 - val_acc: 0.6318\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 14s 343us/sample - loss: 0.9706 - acc: 0.6590 - val_loss: 1.0295 - val_acc: 0.6401\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 13s 333us/sample - loss: 0.9516 - acc: 0.6675 - val_loss: 0.9925 - val_acc: 0.6489\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 13s 317us/sample - loss: 0.9351 - acc: 0.6695 - val_loss: 1.0145 - val_acc: 0.6431\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 13s 330us/sample - loss: 0.9215 - acc: 0.6761 - val_loss: 1.0037 - val_acc: 0.6470\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 13s 317us/sample - loss: 0.9119 - acc: 0.6786 - val_loss: 0.9969 - val_acc: 0.6551\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 14s 341us/sample - loss: 0.8979 - acc: 0.6870 - val_loss: 0.9751 - val_acc: 0.6577\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 13s 328us/sample - loss: 0.8897 - acc: 0.6887 - val_loss: 0.9682 - val_acc: 0.6629\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 14s 343us/sample - loss: 0.8770 - acc: 0.6946 - val_loss: 0.9485 - val_acc: 0.6681\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 14s 340us/sample - loss: 0.8684 - acc: 0.6973 - val_loss: 0.9419 - val_acc: 0.6668\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 13s 333us/sample - loss: 0.8609 - acc: 0.7014 - val_loss: 0.9282 - val_acc: 0.6765\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 13s 334us/sample - loss: 0.8532 - acc: 0.7030 - val_loss: 0.9482 - val_acc: 0.6667\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 14s 342us/sample - loss: 0.8493 - acc: 0.7041 - val_loss: 0.9143 - val_acc: 0.6804\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 14s 355us/sample - loss: 0.8438 - acc: 0.7053 - val_loss: 0.9012 - val_acc: 0.6806\n",
      "Epoch 23/30\n",
      "40000/40000 [==============================] - 14s 347us/sample - loss: 0.8364 - acc: 0.7064 - val_loss: 0.9253 - val_acc: 0.6781\n",
      "Epoch 24/30\n",
      "40000/40000 [==============================] - 13s 337us/sample - loss: 0.8310 - acc: 0.7086 - val_loss: 0.9028 - val_acc: 0.6863\n",
      "Epoch 25/30\n",
      "40000/40000 [==============================] - 13s 321us/sample - loss: 0.8261 - acc: 0.7117 - val_loss: 0.9741 - val_acc: 0.6632\n",
      "Epoch 26/30\n",
      "40000/40000 [==============================] - 13s 327us/sample - loss: 0.8189 - acc: 0.7126 - val_loss: 0.9021 - val_acc: 0.6879\n",
      "Epoch 27/30\n",
      "40000/40000 [==============================] - 14s 344us/sample - loss: 0.8189 - acc: 0.7138 - val_loss: 0.8889 - val_acc: 0.6935\n",
      "Epoch 28/30\n",
      "40000/40000 [==============================] - 12s 293us/sample - loss: 0.8090 - acc: 0.7165 - val_loss: 0.9123 - val_acc: 0.6870\n",
      "Epoch 29/30\n",
      "40000/40000 [==============================] - 14s 347us/sample - loss: 0.8077 - acc: 0.7196 - val_loss: 0.8898 - val_acc: 0.6895\n",
      "Epoch 30/30\n",
      "40000/40000 [==============================] - 14s 341us/sample - loss: 0.8041 - acc: 0.7193 - val_loss: 0.8956 - val_acc: 0.6881\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 64x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "weight_decay = 0.0001\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 293,034\n",
      "Trainable params: 292,586\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 13s 337us/sample - loss: 1.8138 - acc: 0.4107 - val_loss: 1.3153 - val_acc: 0.5414\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 12s 312us/sample - loss: 1.2104 - acc: 0.5878 - val_loss: 1.0558 - val_acc: 0.6450\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 13s 321us/sample - loss: 0.9790 - acc: 0.6728 - val_loss: 0.8977 - val_acc: 0.7020\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 13s 319us/sample - loss: 0.8582 - acc: 0.7167 - val_loss: 0.7853 - val_acc: 0.7456\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 13s 330us/sample - loss: 0.7836 - acc: 0.7452 - val_loss: 0.8113 - val_acc: 0.7421\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 13s 336us/sample - loss: 0.7277 - acc: 0.7697 - val_loss: 0.7667 - val_acc: 0.7577\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 13s 325us/sample - loss: 0.6935 - acc: 0.7819 - val_loss: 0.7929 - val_acc: 0.7547\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 13s 316us/sample - loss: 0.6622 - acc: 0.7979 - val_loss: 0.7382 - val_acc: 0.7807\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 13s 322us/sample - loss: 0.6434 - acc: 0.8085 - val_loss: 0.6495 - val_acc: 0.8069\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 13s 327us/sample - loss: 0.6177 - acc: 0.8171 - val_loss: 0.6514 - val_acc: 0.8094\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 13s 321us/sample - loss: 0.6033 - acc: 0.8248 - val_loss: 0.6806 - val_acc: 0.8047\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 13s 327us/sample - loss: 0.5877 - acc: 0.8338 - val_loss: 0.6543 - val_acc: 0.8151\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 13s 317us/sample - loss: 0.5705 - acc: 0.8421 - val_loss: 0.6990 - val_acc: 0.8096\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 13s 319us/sample - loss: 0.5613 - acc: 0.8483 - val_loss: 0.6838 - val_acc: 0.8138\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 13s 324us/sample - loss: 0.5537 - acc: 0.8514 - val_loss: 0.6427 - val_acc: 0.8256\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 13s 331us/sample - loss: 0.5489 - acc: 0.8554 - val_loss: 0.6601 - val_acc: 0.8258\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 13s 323us/sample - loss: 0.5345 - acc: 0.8608 - val_loss: 0.6722 - val_acc: 0.8261\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 13s 330us/sample - loss: 0.5293 - acc: 0.8655 - val_loss: 0.6856 - val_acc: 0.8245\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 13s 332us/sample - loss: 0.5213 - acc: 0.8684 - val_loss: 0.6821 - val_acc: 0.8210\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 13s 324us/sample - loss: 0.5165 - acc: 0.8724 - val_loss: 0.6668 - val_acc: 0.8306\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 13s 334us/sample - loss: 0.5144 - acc: 0.8745 - val_loss: 0.6595 - val_acc: 0.8369\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 13s 321us/sample - loss: 0.5024 - acc: 0.8795 - val_loss: 0.6669 - val_acc: 0.8366\n",
      "Epoch 23/30\n",
      "40000/40000 [==============================] - 13s 330us/sample - loss: 0.5059 - acc: 0.8787 - val_loss: 0.6875 - val_acc: 0.8315\n",
      "Epoch 24/30\n",
      "40000/40000 [==============================] - 13s 320us/sample - loss: 0.4988 - acc: 0.8832 - val_loss: 0.7138 - val_acc: 0.8266\n",
      "Epoch 25/30\n",
      "40000/40000 [==============================] - 13s 324us/sample - loss: 0.4937 - acc: 0.8850 - val_loss: 0.7286 - val_acc: 0.8228\n",
      "Epoch 26/30\n",
      "40000/40000 [==============================] - 13s 331us/sample - loss: 0.4909 - acc: 0.8887 - val_loss: 0.7286 - val_acc: 0.8306\n",
      "Epoch 27/30\n",
      "40000/40000 [==============================] - 13s 318us/sample - loss: 0.4894 - acc: 0.8894 - val_loss: 0.7036 - val_acc: 0.8287\n",
      "Epoch 28/30\n",
      "40000/40000 [==============================] - 13s 314us/sample - loss: 0.4833 - acc: 0.8927 - val_loss: 0.6765 - val_acc: 0.8367\n",
      "Epoch 29/30\n",
      "40000/40000 [==============================] - 13s 315us/sample - loss: 0.4820 - acc: 0.8925 - val_loss: 0.7180 - val_acc: 0.8253\n",
      "Epoch 30/30\n",
      "40000/40000 [==============================] - 13s 322us/sample - loss: 0.4717 - acc: 0.8987 - val_loss: 0.7013 - val_acc: 0.8358\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=30, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
