{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "* All models run on a single GTX 1060 3GB, i5 8600k and 16GB RAM\n",
    "* Purpose of the experiments is to get practical experience in building simple models and working with established datasets\n",
    "* In the future, I would like to work with more complex models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unpickling helper function to read from  file and return data\n",
    "'''\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load pickled CIFAR-10 Data\n",
    "'''\n",
    "def load_all_cifar_data():\n",
    "    folder_path = 'cifar-10-batches-py'\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f'{folder_path}/data_batch_{i}')\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "    \n",
    "    test_data = unpickle(f'{folder_path}/test_batch')\n",
    "    \n",
    "    return np.concatenate(tuple(data), axis=0), np.concatenate(tuple(labels), axis=0), test_data[b'data'], test_data[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_for_cnn(data, rows=50000):\n",
    "    num_entries_in_channel = int(data.shape[1] / 3)\n",
    "    data_4d = np.zeros((rows,3,32,32))\n",
    "    for i in range(rows):\n",
    "        x = data[i]\n",
    "        r_1d = x[0:num_entries_in_channel]\n",
    "        g_1d = x[num_entries_in_channel:2*num_entries_in_channel]\n",
    "        b_1d = x[num_entries_in_channel*2:]\n",
    "\n",
    "        r_2d = np.reshape(r_1d, (32,32))\n",
    "        g_2d = np.reshape(g_1d, (32,32))\n",
    "        b_2d = np.reshape(b_1d, (32,32))\n",
    "        \n",
    "        image_3d = np.zeros((3,32,32))\n",
    "        image_3d[0] = r_2d\n",
    "        image_3d[1] = g_2d\n",
    "        image_3d[2] = b_2d\n",
    "                \n",
    "        data_4d[i] = image_3d\n",
    "    print(data_4d.shape)\n",
    "    data_4d = np.swapaxes(data_4d,1,3)\n",
    "    data_4d = np.swapaxes(data_4d, 1,2)\n",
    "    \n",
    "    mean = np.mean(data_4d,axis=(0,1,2,3))\n",
    "    std = np.std(data_4d,axis=(0,1,2,3))\n",
    "    data_4d = (data_4d-mean)/(std+1e-7)\n",
    "    return data_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, test_x, test_y = load_all_cifar_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "one_hot_placeholder = np.zeros((50000,10))\n",
    "one_hot_placeholder[np.arange(50000), labels] = 1 # One hot conversion for labeled data\n",
    "labels = one_hot_placeholder\n",
    "\n",
    "one_hot_placeholder = np.zeros((10000,10))\n",
    "one_hot_placeholder[np.arange(10000), test_y] = 1 # One hot conversion for labeled data\n",
    "test_y = one_hot_placeholder\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.divide(data, 255) # normalize all image data\n",
    "train_x = np.divide(test_x, 255)\n",
    "import os \n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Plain neural network architeture fed with a flattened version of the images. \n",
    "* Achieved around 50% accuracy which is clearly better than random (10%) but still can be improved\n",
    "\n",
    "### Benifits\n",
    "* Fast to train due to its simplicity\n",
    "* Easy conceptual model for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "from tensorflow.keras.layers import Dense\n",
    "model.add(Dense(units=1024, activation='relu', input_dim=3072))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, labels, validation_split=0.20, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/simple-nn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Proven to be better technique when working with images hence, much better performance that simple NN\n",
    "* Achieves validation accuracy of 75% in a fairly small number of epochs\n",
    "\n",
    "### Benifits\n",
    "* Reuse of kernels/weights that are learnt and convolved over the image, thus allowing the model to learn features\n",
    "* Smaller number of parameters than NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/simple-cnn-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN WITH SEPERABLE CONVOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Much less amount of computations involved due to use of depthwise and pointwise convolutions\n",
    "* Still achieved validation accuracy similar to plain CNN\n",
    "* Efficient and Faster for inference and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, DepthwiseConv2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, kernel_size=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, kernel_size=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/seperable-cnn-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 64x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Much more params that any other CNN thus a more complex of a model\n",
    "* Using kernel regularizer of 0.0001 to prevent high variance, tried using 0.001 but achieved worse results\n",
    "* Best model in all of the experiments in terms of validation accuracy\n",
    "* Takes longer to train overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "weight_decay = 0.0001\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(convert_data_for_cnn(data), labels, validation_split=0.2, epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/deep-cnn-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Analysis\n",
    "### Summary:\n",
    "* As expected, depp-cnn-model (64x3 CNN) has the highest test accuracy of 83%\n",
    "* Better results should be achieveable by using data augmentation as well as more complex architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import os\n",
    "    \n",
    "    all_models = []\n",
    "    for model in os.listdir('models'):\n",
    "        all_models.append((model,load_model('models/'+model)))\n",
    "    \n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(f'Evaluating {model[0]}')\n",
    "    if len(model[1].input_shape) == 4:\n",
    "        model[1].evaluate(x=convert_data_for_cnn(test_x,10000), y=test_y)\n",
    "    else:\n",
    "        model[1].evaluate(x=test_x, y=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v1(input_shape=(32,32,3), depth=20)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000, 3, 32, 32)\n",
      "(10000, 3, 32, 32)\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 1.5679 - acc: 0.4937 - val_loss: 1.5535 - val_acc: 0.5208\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 1.1606 - acc: 0.6460 - val_loss: 1.2327 - val_acc: 0.6336\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 1.0033 - acc: 0.7074 - val_loss: 1.0575 - val_acc: 0.7010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9138 - acc: 0.7402 - val_loss: 1.2441 - val_acc: 0.6465\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.8558 - acc: 0.7642 - val_loss: 1.7199 - val_acc: 0.5916\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.8147 - acc: 0.7787 - val_loss: 1.2710 - val_acc: 0.6537\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.7762 - acc: 0.7956 - val_loss: 0.8834 - val_acc: 0.7575\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.7466 - acc: 0.8033 - val_loss: 0.9283 - val_acc: 0.7546\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.7255 - acc: 0.8130 - val_loss: 0.8772 - val_acc: 0.7670\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.7072 - acc: 0.8187 - val_loss: 0.9660 - val_acc: 0.7468\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6918 - acc: 0.8259 - val_loss: 1.1129 - val_acc: 0.7186\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6750 - acc: 0.8309 - val_loss: 0.8748 - val_acc: 0.7644\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.6624 - acc: 0.8371 - val_loss: 0.9024 - val_acc: 0.7636\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6486 - acc: 0.8437 - val_loss: 0.9982 - val_acc: 0.7507\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6429 - acc: 0.8448 - val_loss: 0.7986 - val_acc: 0.8018\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6345 - acc: 0.8492 - val_loss: 0.8070 - val_acc: 0.7955\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6256 - acc: 0.8510 - val_loss: 0.8143 - val_acc: 0.7995\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6155 - acc: 0.8551 - val_loss: 0.6806 - val_acc: 0.8381\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.6079 - acc: 0.8585 - val_loss: 0.7730 - val_acc: 0.8116\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.6123 - acc: 0.8571 - val_loss: 0.7320 - val_acc: 0.8236\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5967 - acc: 0.8637 - val_loss: 0.7983 - val_acc: 0.8110\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5946 - acc: 0.8642 - val_loss: 0.7819 - val_acc: 0.8154\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5877 - acc: 0.8673 - val_loss: 0.7671 - val_acc: 0.8095\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5855 - acc: 0.8678 - val_loss: 0.7028 - val_acc: 0.8368\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5764 - acc: 0.8709 - val_loss: 0.8003 - val_acc: 0.8048\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5767 - acc: 0.8702 - val_loss: 0.7334 - val_acc: 0.8240\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5703 - acc: 0.8724 - val_loss: 0.8502 - val_acc: 0.7942\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5666 - acc: 0.8746 - val_loss: 0.8783 - val_acc: 0.7928\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5620 - acc: 0.8764 - val_loss: 0.7899 - val_acc: 0.8097\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5635 - acc: 0.8754 - val_loss: 0.9452 - val_acc: 0.7800\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5545 - acc: 0.8803 - val_loss: 0.9312 - val_acc: 0.7765\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5553 - acc: 0.8802 - val_loss: 0.8152 - val_acc: 0.7992\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5517 - acc: 0.8792 - val_loss: 0.6691 - val_acc: 0.8451\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5494 - acc: 0.8824 - val_loss: 0.7052 - val_acc: 0.8339\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5448 - acc: 0.8835 - val_loss: 0.8072 - val_acc: 0.8133\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5473 - acc: 0.8829 - val_loss: 0.7024 - val_acc: 0.8342\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5428 - acc: 0.8835 - val_loss: 0.8530 - val_acc: 0.7967\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5409 - acc: 0.8835 - val_loss: 0.9775 - val_acc: 0.7854\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5365 - acc: 0.8869 - val_loss: 0.8824 - val_acc: 0.7936\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5399 - acc: 0.8841 - val_loss: 0.7469 - val_acc: 0.8255\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5305 - acc: 0.8876 - val_loss: 0.7028 - val_acc: 0.8405\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5284 - acc: 0.8889 - val_loss: 0.6697 - val_acc: 0.8506\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5313 - acc: 0.8883 - val_loss: 0.6719 - val_acc: 0.8466\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5304 - acc: 0.8878 - val_loss: 0.7315 - val_acc: 0.8281\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5258 - acc: 0.8900 - val_loss: 0.8018 - val_acc: 0.8113\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5210 - acc: 0.8924 - val_loss: 0.6329 - val_acc: 0.8597\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5248 - acc: 0.8901 - val_loss: 0.6183 - val_acc: 0.8641\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 37s 23ms/step - loss: 0.5250 - acc: 0.8892 - val_loss: 0.6093 - val_acc: 0.8661\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5205 - acc: 0.8915 - val_loss: 0.7575 - val_acc: 0.8277\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5197 - acc: 0.8923 - val_loss: 0.8176 - val_acc: 0.8103\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5138 - acc: 0.8938 - val_loss: 0.7002 - val_acc: 0.8391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.5162 - acc: 0.8940 - val_loss: 0.6661 - val_acc: 0.8533\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5136 - acc: 0.8932 - val_loss: 0.6601 - val_acc: 0.8520\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5142 - acc: 0.8918 - val_loss: 0.8634 - val_acc: 0.8007\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5120 - acc: 0.8957 - val_loss: 0.7058 - val_acc: 0.8427\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5088 - acc: 0.8962 - val_loss: 0.6429 - val_acc: 0.8558\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5114 - acc: 0.8942 - val_loss: 0.6901 - val_acc: 0.8466\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5129 - acc: 0.8956 - val_loss: 0.7769 - val_acc: 0.8217\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5065 - acc: 0.8976 - val_loss: 0.6170 - val_acc: 0.8670\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5083 - acc: 0.8965 - val_loss: 0.6143 - val_acc: 0.8662\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5036 - acc: 0.8984 - val_loss: 0.7439 - val_acc: 0.8287\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5079 - acc: 0.8971 - val_loss: 0.7199 - val_acc: 0.8383\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5036 - acc: 0.8976 - val_loss: 0.6783 - val_acc: 0.8483\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5033 - acc: 0.8994 - val_loss: 0.7520 - val_acc: 0.8297\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5039 - acc: 0.8988 - val_loss: 0.6592 - val_acc: 0.8565\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5018 - acc: 0.8993 - val_loss: 0.6919 - val_acc: 0.8477\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5015 - acc: 0.8990 - val_loss: 0.6799 - val_acc: 0.8539\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4996 - acc: 0.9009 - val_loss: 0.6358 - val_acc: 0.8580\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.4976 - acc: 0.9011 - val_loss: 0.8226 - val_acc: 0.8107\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4994 - acc: 0.9007 - val_loss: 0.8257 - val_acc: 0.8252\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.5000 - acc: 0.8999 - val_loss: 0.6129 - val_acc: 0.8672\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4969 - acc: 0.8995 - val_loss: 0.7503 - val_acc: 0.8258\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.4944 - acc: 0.9014 - val_loss: 0.6849 - val_acc: 0.8466\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.4985 - acc: 0.8997 - val_loss: 0.6586 - val_acc: 0.8561\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4959 - acc: 0.9009 - val_loss: 0.7320 - val_acc: 0.8383\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4996 - acc: 0.8993 - val_loss: 0.6269 - val_acc: 0.8634\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.4910 - acc: 0.9018 - val_loss: 0.7369 - val_acc: 0.8415\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4928 - acc: 0.9009 - val_loss: 0.6677 - val_acc: 0.8553\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4881 - acc: 0.9037 - val_loss: 0.7469 - val_acc: 0.8324\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.4913 - acc: 0.9038 - val_loss: 0.7079 - val_acc: 0.8437\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4851 - acc: 0.9041 - val_loss: 0.7256 - val_acc: 0.8414\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4077 - acc: 0.9321 - val_loss: 0.5176 - val_acc: 0.8985\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.3742 - acc: 0.9419 - val_loss: 0.4824 - val_acc: 0.9089\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3566 - acc: 0.9467 - val_loss: 0.4767 - val_acc: 0.9101\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3442 - acc: 0.9495 - val_loss: 0.4791 - val_acc: 0.9101\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3334 - acc: 0.9519 - val_loss: 0.4687 - val_acc: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.3294 - acc: 0.9516 - val_loss: 0.4657 - val_acc: 0.9121\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.3184 - acc: 0.9545 - val_loss: 0.4668 - val_acc: 0.9116\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.3129 - acc: 0.9545 - val_loss: 0.4592 - val_acc: 0.9116\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3069 - acc: 0.9556 - val_loss: 0.4571 - val_acc: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3018 - acc: 0.9565 - val_loss: 0.4513 - val_acc: 0.9153\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2941 - acc: 0.9585 - val_loss: 0.4614 - val_acc: 0.9120\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2884 - acc: 0.9601 - val_loss: 0.4594 - val_acc: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2825 - acc: 0.9607 - val_loss: 0.4625 - val_acc: 0.9105\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2804 - acc: 0.9599 - val_loss: 0.4559 - val_acc: 0.9122\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2758 - acc: 0.9612 - val_loss: 0.4596 - val_acc: 0.9084\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2697 - acc: 0.9635 - val_loss: 0.4513 - val_acc: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2651 - acc: 0.9653 - val_loss: 0.4430 - val_acc: 0.9136\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2632 - acc: 0.9640 - val_loss: 0.4458 - val_acc: 0.9139\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2535 - acc: 0.9672 - val_loss: 0.4374 - val_acc: 0.9138\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2532 - acc: 0.9677 - val_loss: 0.4522 - val_acc: 0.9132\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2506 - acc: 0.9667 - val_loss: 0.4544 - val_acc: 0.9118\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2481 - acc: 0.9668 - val_loss: 0.4425 - val_acc: 0.9162\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2438 - acc: 0.9672 - val_loss: 0.4566 - val_acc: 0.9102\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2409 - acc: 0.9684 - val_loss: 0.4456 - val_acc: 0.9122\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2356 - acc: 0.9690 - val_loss: 0.4455 - val_acc: 0.9123\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2353 - acc: 0.9693 - val_loss: 0.4485 - val_acc: 0.9124\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2294 - acc: 0.9705 - val_loss: 0.4570 - val_acc: 0.9098\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2311 - acc: 0.9693 - val_loss: 0.4491 - val_acc: 0.9117\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2262 - acc: 0.9703 - val_loss: 0.4611 - val_acc: 0.9084\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2253 - acc: 0.9699 - val_loss: 0.4570 - val_acc: 0.9108\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2226 - acc: 0.9706 - val_loss: 0.4463 - val_acc: 0.9132\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2216 - acc: 0.9723 - val_loss: 0.4354 - val_acc: 0.9187\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2161 - acc: 0.9723 - val_loss: 0.4595 - val_acc: 0.9094\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2157 - acc: 0.9719 - val_loss: 0.4476 - val_acc: 0.9104\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2130 - acc: 0.9726 - val_loss: 0.4448 - val_acc: 0.9131\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2105 - acc: 0.9731 - val_loss: 0.4643 - val_acc: 0.9100\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2077 - acc: 0.9737 - val_loss: 0.4451 - val_acc: 0.9087\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2072 - acc: 0.9729 - val_loss: 0.4582 - val_acc: 0.9107\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2036 - acc: 0.9740 - val_loss: 0.4408 - val_acc: 0.9126\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.2022 - acc: 0.9750 - val_loss: 0.4411 - val_acc: 0.9144\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1939 - acc: 0.9777 - val_loss: 0.4317 - val_acc: 0.9155\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1907 - acc: 0.9783 - val_loss: 0.4322 - val_acc: 0.9156\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1900 - acc: 0.9799 - val_loss: 0.4309 - val_acc: 0.9151\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1874 - acc: 0.9803 - val_loss: 0.4293 - val_acc: 0.9166\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1859 - acc: 0.9804 - val_loss: 0.4300 - val_acc: 0.9157\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1834 - acc: 0.9811 - val_loss: 0.4290 - val_acc: 0.9170\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1854 - acc: 0.9804 - val_loss: 0.4311 - val_acc: 0.9161\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1859 - acc: 0.9810 - val_loss: 0.4283 - val_acc: 0.9166\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1832 - acc: 0.9815 - val_loss: 0.4303 - val_acc: 0.9169\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1820 - acc: 0.9822 - val_loss: 0.4295 - val_acc: 0.9171\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1803 - acc: 0.9821 - val_loss: 0.4259 - val_acc: 0.9167\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1816 - acc: 0.9819 - val_loss: 0.4313 - val_acc: 0.9168\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1815 - acc: 0.9815 - val_loss: 0.4265 - val_acc: 0.9184\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1788 - acc: 0.9832 - val_loss: 0.4266 - val_acc: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1806 - acc: 0.9821 - val_loss: 0.4276 - val_acc: 0.9183\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1785 - acc: 0.9821 - val_loss: 0.4293 - val_acc: 0.9172\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1792 - acc: 0.9824 - val_loss: 0.4276 - val_acc: 0.9175\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1782 - acc: 0.9827 - val_loss: 0.4275 - val_acc: 0.9176\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1767 - acc: 0.9833 - val_loss: 0.4263 - val_acc: 0.9178\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1785 - acc: 0.9822 - val_loss: 0.4280 - val_acc: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1768 - acc: 0.9828 - val_loss: 0.4274 - val_acc: 0.9177\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1751 - acc: 0.9843 - val_loss: 0.4274 - val_acc: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1779 - acc: 0.9829 - val_loss: 0.4306 - val_acc: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1769 - acc: 0.9829 - val_loss: 0.4301 - val_acc: 0.9181\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1771 - acc: 0.9829 - val_loss: 0.4308 - val_acc: 0.9173\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1752 - acc: 0.9835 - val_loss: 0.4281 - val_acc: 0.9187\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1747 - acc: 0.9835 - val_loss: 0.4292 - val_acc: 0.9184\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1732 - acc: 0.9842 - val_loss: 0.4305 - val_acc: 0.9173\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1750 - acc: 0.9829 - val_loss: 0.4336 - val_acc: 0.9169\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1743 - acc: 0.9832 - val_loss: 0.4316 - val_acc: 0.9164\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1719 - acc: 0.9842 - val_loss: 0.4295 - val_acc: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1743 - acc: 0.9828 - val_loss: 0.4291 - val_acc: 0.9179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-05\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1724 - acc: 0.9847 - val_loss: 0.4290 - val_acc: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1739 - acc: 0.9834 - val_loss: 0.4290 - val_acc: 0.9180\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1734 - acc: 0.9839 - val_loss: 0.4301 - val_acc: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1722 - acc: 0.9837 - val_loss: 0.4304 - val_acc: 0.9179\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1718 - acc: 0.9843 - val_loss: 0.4300 - val_acc: 0.9189\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1696 - acc: 0.9853 - val_loss: 0.4310 - val_acc: 0.9192\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1696 - acc: 0.9849 - val_loss: 0.4338 - val_acc: 0.9171\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1702 - acc: 0.9846 - val_loss: 0.4317 - val_acc: 0.9174\n",
      "Learning rate:  1e-06\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1692 - acc: 0.9852 - val_loss: 0.4317 - val_acc: 0.9175\n",
      "Learning rate:  1e-06\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1693 - acc: 0.9844 - val_loss: 0.4307 - val_acc: 0.9176\n",
      "Learning rate:  1e-06\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1691 - acc: 0.9846 - val_loss: 0.4279 - val_acc: 0.9185\n",
      "Learning rate:  1e-06\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1678 - acc: 0.9855 - val_loss: 0.4288 - val_acc: 0.9187\n",
      "Learning rate:  1e-06\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1683 - acc: 0.9851 - val_loss: 0.4293 - val_acc: 0.9187\n",
      "Learning rate:  1e-06\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1691 - acc: 0.9843 - val_loss: 0.4310 - val_acc: 0.9175\n",
      "Learning rate:  1e-06\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1700 - acc: 0.9839 - val_loss: 0.4300 - val_acc: 0.9189\n",
      "Learning rate:  1e-06\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1690 - acc: 0.9849 - val_loss: 0.4304 - val_acc: 0.9185\n",
      "Learning rate:  1e-06\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1692 - acc: 0.9843 - val_loss: 0.4296 - val_acc: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1692 - acc: 0.9850 - val_loss: 0.4301 - val_acc: 0.9189\n",
      "Learning rate:  1e-06\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1689 - acc: 0.9844 - val_loss: 0.4306 - val_acc: 0.9185\n",
      "Learning rate:  1e-06\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1695 - acc: 0.9845 - val_loss: 0.4301 - val_acc: 0.9184\n",
      "Learning rate:  1e-06\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1693 - acc: 0.9849 - val_loss: 0.4320 - val_acc: 0.9178\n",
      "Learning rate:  1e-06\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1684 - acc: 0.9856 - val_loss: 0.4292 - val_acc: 0.9185\n",
      "Learning rate:  1e-06\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1706 - acc: 0.9843 - val_loss: 0.4304 - val_acc: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1669 - acc: 0.9861 - val_loss: 0.4310 - val_acc: 0.9186\n",
      "Learning rate:  1e-06\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1676 - acc: 0.9853 - val_loss: 0.4300 - val_acc: 0.9186\n",
      "Learning rate:  1e-06\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1677 - acc: 0.9849 - val_loss: 0.4310 - val_acc: 0.9177\n",
      "Learning rate:  1e-06\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1672 - acc: 0.9854 - val_loss: 0.4288 - val_acc: 0.9183\n",
      "Learning rate:  1e-06\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1686 - acc: 0.9845 - val_loss: 0.4299 - val_acc: 0.9187\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1697 - acc: 0.9845 - val_loss: 0.4298 - val_acc: 0.9181\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1671 - acc: 0.9858 - val_loss: 0.4300 - val_acc: 0.9185\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1688 - acc: 0.9846 - val_loss: 0.4306 - val_acc: 0.9179\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1682 - acc: 0.9851 - val_loss: 0.4299 - val_acc: 0.9187\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1691 - acc: 0.9845 - val_loss: 0.4315 - val_acc: 0.9192\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1670 - acc: 0.9854 - val_loss: 0.4302 - val_acc: 0.9187\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1670 - acc: 0.9856 - val_loss: 0.4285 - val_acc: 0.9195\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1692 - acc: 0.9841 - val_loss: 0.4301 - val_acc: 0.9184\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1690 - acc: 0.9845 - val_loss: 0.4320 - val_acc: 0.9177\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1697 - acc: 0.9847 - val_loss: 0.4305 - val_acc: 0.9185\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1678 - acc: 0.9855 - val_loss: 0.4292 - val_acc: 0.9193\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.1681 - acc: 0.9847 - val_loss: 0.4308 - val_acc: 0.9186\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1674 - acc: 0.9853 - val_loss: 0.4295 - val_acc: 0.9186\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1684 - acc: 0.9847 - val_loss: 0.4297 - val_acc: 0.9191\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1688 - acc: 0.9841 - val_loss: 0.4295 - val_acc: 0.9192\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1688 - acc: 0.9850 - val_loss: 0.4300 - val_acc: 0.9197\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1673 - acc: 0.9849 - val_loss: 0.4298 - val_acc: 0.9188\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1667 - acc: 0.9849 - val_loss: 0.4300 - val_acc: 0.9191\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.1664 - acc: 0.9853 - val_loss: 0.4284 - val_acc: 0.9192\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(convert_data_for_cnn(data))\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "history = model.fit_generator(datagen.flow(convert_data_for_cnn(data), labels, batch_size=32),\n",
    "                    validation_data=(convert_data_for_cnn(test_x,10000), test_y),\n",
    "                    epochs=200, verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# history = model.fit(convert_data_for_cnn(data), labels,\n",
    "#               batch_size=32,\n",
    "#               epochs=50,\n",
    "#               validation_split=0.2,\n",
    "#               shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/resnet-20-200-epochs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
